{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c5db6a-2ed4-4050-aef4-81d10bb51331",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sample_data.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 1. Load & initial cleanup\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_data.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, skiprows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m      8\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEND DATE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEND DATE\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(\n\u001b[0;32m    496\u001b[0m         io,\n\u001b[0;32m    497\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    498\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    499\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m    500\u001b[0m     )\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1551\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m   1552\u001b[0m     )\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1403\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_data.xlsx'"
     ]
    }
   ],
   "source": [
    "# promo_event_analysis_pipeline.ipynb (or .py)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load & initial cleanup\n",
    "df = pd.read_excel(\"sample_data.xlsx\", skiprows=8)\n",
    "df.columns = df.columns.str.strip()\n",
    "df['END DATE'] = pd.to_datetime(df['END DATE'], errors='coerce')\n",
    "\n",
    "# 2. Ensure numeric prices & create SKU key\n",
    "price_cols = ['NON-PROMO PRICE', 'PROMO PRICE', 'AVG PRICE']\n",
    "df[price_cols] = df[price_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "df['Brand_Pack_PackType'] = (\n",
    "    df['BRAND'] + ' x ' +\n",
    "    df['ACTUAL PACK SIZE'].astype(str) + ' x ' +\n",
    "    df['PACK TYPE'].astype(str)\n",
    ")\n",
    "\n",
    "# 3. Baseline price (80th percentile non-promo price)\n",
    "baseline_price = (\n",
    "    df.groupby(['Markets','YEAR','Brand_Pack_PackType'])['NON-PROMO PRICE']\n",
    "      .quantile(0.8)\n",
    "      .reset_index(name='Baseline Price')\n",
    ")\n",
    "df = df.merge(baseline_price, on=['Markets','YEAR','Brand_Pack_PackType'], how='left')\n",
    "\n",
    "# 4. Identify promo weeks\n",
    "df['Promo Vol Check'] = (df['Sales Units Any Promo'] > 0.5 * df['Sales Units']).map({True:'Y', False:'N'})\n",
    "df['Promo 5% Deviation Check'] = (df['PROMO PRICE'] < 0.95 * df['NON-PROMO PRICE']).map({True:'Y', False:'N'})\n",
    "df['Promo Week Check'] = ((df['Promo Vol Check']=='Y') & (df['Promo 5% Deviation Check']=='Y')).map({True:'Y', False:'N'})\n",
    "\n",
    "# 5. Baseline volume (mean non-promo units)\n",
    "df['Sales Units No Promo'] = pd.to_numeric(df['Sales Units No Promo'], errors='coerce')\n",
    "non_promo = df[df['Promo Week Check']=='N']\n",
    "baseline_vol = (\n",
    "    non_promo.groupby(['Markets','YEAR','Brand_Pack_PackType'])['Sales Units No Promo']\n",
    "             .mean()\n",
    "             .reset_index(name='Baseline Volume')\n",
    ")\n",
    "df = df.merge(baseline_vol, on=['Markets','YEAR','Brand_Pack_PackType'], how='left')\n",
    "\n",
    "# 6. Filter anomalies & compute uplift\n",
    "df['Erase Anomaly'] = ((df['Baseline Volume']>df['Sales Units']) & (df['Baseline Price']<df['PROMO PRICE'])).map({True:'Y', False:'N'})\n",
    "df['Uplift'] = df.apply(\n",
    "    lambda r: r['Sales Units'] - r['Baseline Volume']\n",
    "              if (r['Promo Week Check']=='Y') and (r['Erase Anomaly']=='N')\n",
    "              else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 7. Build valid promo-week table\n",
    "valid = df[(df['Promo Week Check']=='Y') & (df['Erase Anomaly']=='N')].copy()\n",
    "valid.sort_values(['Markets','YEAR','Brand_Pack_PackType','END DATE'], inplace=True)\n",
    "valid['Date_Gap'] = valid.groupby(['Markets','YEAR','Brand_Pack_PackType'])['END DATE'].diff().dt.days.fillna(0)\n",
    "valid['New_Event'] = (valid['Date_Gap']>7).astype(int)\n",
    "valid['Event_ID'] = valid.groupby(['Markets','YEAR','Brand_Pack_PackType'])['New_Event'].cumsum()\n",
    "valid['Valid Event Uplift'] = ((valid['Uplift'].notna()) & (valid['Uplift']>0.2*valid['Baseline Volume'])).map({True:'Y',False:'N'})\n",
    "valid = valid[valid['Valid Event Uplift']=='Y']\n",
    "\n",
    "# 8. Compute promo depth & bucket per event\n",
    "valid['Promo Depth %'] = 1 - (valid['PROMO PRICE']/valid['Baseline Price'])\n",
    "event_depth = (\n",
    "    valid.groupby(['Markets','YEAR','Brand_Pack_PackType','Event_ID'])['Promo Depth %']\n",
    "         .median()\n",
    "         .reset_index()\n",
    ")\n",
    "bins = [0.05,0.10,0.15,0.20,0.25,0.30,0.35,0.40,0.45,0.50,0.55,0.60,0.65,0.70,0.75,0.80,1.10]\n",
    "labels = ['5–9.99%','10–14.99%','15–19.99%','20–24.99%','25–29.99%','30–34.99%',\n",
    "          '35–39.99%','40–44.99%','45–49.99%','50–54.99%','55–59.99%','60–64.99%',\n",
    "          '65–69.99%','70–74.99%','75–79.99%','80%+']\n",
    "event_depth['Depth Bucket'] = pd.cut(event_depth['Promo Depth %'], bins=bins, labels=labels, right=False)\n",
    "valid = valid.merge(event_depth, on=['Markets','YEAR','Brand_Pack_PackType','Event_ID'], how='left')\n",
    "\n",
    "# 9. Compute event duration & buckets\n",
    "event_duration = valid.groupby(['Markets','YEAR','Brand_Pack_PackType','Event_ID']).size().reset_index(name='Event Duration (weeks)')\n",
    "valid = valid.merge(event_duration, on=['Markets','YEAR','Brand_Pack_PackType','Event_ID'], how='left')\n",
    "\n",
    "event_stats = (\n",
    "    valid.groupby(['Markets','YEAR','Brand_Pack_PackType','Event_ID'])\n",
    "         .agg(Event_Duration=('Event_ID','size'),\n",
    "              Max_Depth=('Promo Depth %','max'))\n",
    "         .reset_index()\n",
    ")\n",
    "event_stats['Duration Bucket'] = pd.cut(event_stats['Event_Duration'], bins=[0,3,6,np.inf], labels=['Short','Medium','Long'], right=True)\n",
    "event_stats['Depth Bucket'] = pd.cut(event_stats['Max_Depth'], bins=[-1,0.10,0.20,1.10], labels=['Shallow','Medium','Deep'], right=False)\n",
    "valid = valid.merge(event_stats[['Markets','YEAR','Brand_Pack_PackType','Event_ID','Duration Bucket','Depth Bucket']], on=['Markets','YEAR','Brand_Pack_PackType','Event_ID'], how='left')\n",
    "\n",
    "# 10. Filter to MAT window (Mar 2024 → Mar 2025)\n",
    "start, end = pd.Timestamp('2024-03-01'), pd.Timestamp('2025-03-31')\n",
    "df = df[(df['END DATE']>=start)&(df['END DATE']<=end)].copy()\n",
    "valid = valid[(valid['END DATE']>=start)&(valid['END DATE']<=end)].copy()\n",
    "\n",
    "# 11. Save outputs\n",
    "df.to_excel(\"sample_data_cleaned.xlsx\", index=False)\n",
    "valid.to_excel(\"sample_data_events.xlsx\", sheet_name=\"Weekly\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3ab7d-e238-4887-948d-0c5bde1f66e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
